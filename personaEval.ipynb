{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85E1pezoQcdq",
        "outputId": "40786613-fd7d-4f37-8e1f-632f56d02a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: icecream in /usr/local/lib/python3.8/dist-packages (2.1.3)\n",
            "Requirement already satisfied: asttokens>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from icecream) (2.2.1)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from icecream) (2.6.1)\n",
            "Requirement already satisfied: executing>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from icecream) (1.2.0)\n",
            "Requirement already satisfied: colorama>=0.3.9 in /usr/local/lib/python3.8/dist-packages (from icecream) (0.4.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from asttokens>=2.0.1->icecream) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.8/dist-packages (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from rouge-score) (3.7)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from rouge-score) (1.21.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->rouge-score) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->rouge-score) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk->rouge-score) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->rouge-score) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install icecream\n",
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/illidanlab/personaGPT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STRtzpWPS3aS",
        "outputId": "df582467-9d74-4c45-e0c1-17a610e2e8f2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'personaGPT' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_Ec6tjtTd_J",
        "outputId": "e67058ac-e0ae-4a7d-9513-16dce556e595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/personaGPT/')"
      ],
      "metadata": {
        "id": "ybgD0Np9U2fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, os, pickle, time\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "YOBGiro7TD-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Oct 12 14:21:35 2020\n",
        "\n",
        "@author: af1tang\n",
        "\"\"\"\n",
        "import os, torch, pickle\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelWithLMHead\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# paths and configs\n",
        "save_path = '/content/pc.t'\n",
        "tokenizer_path = 'af1tang/personaGPT'\n",
        "model_path = 'af1tang/personaGPT'\n",
        "data_path = '/content/personaGPT/data'\n",
        "\n",
        "def create_dir(directory):\n",
        "    \"\"\"create directory if not exists\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "# initialize save folder\n",
        "create_dir(save_path)\n",
        "\n",
        "# global pretrained model and tokenizer\n",
        "def load_from_pretrained():\n",
        "    print(\"*\"*50)\n",
        "    print(\"Load from checkpoint\")\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('af1tang/personaGPT', \n",
        "                                            pad_token='<|endoftext|>', cls_token='<|cls|>',\n",
        "                                            sep_token='<|sep|>')\n",
        "    model = GPT2LMHeadModel.from_pretrained('af1tang/personaGPT')\n",
        "    try:\n",
        "        with open(os.path.join(opts.output_dir, 'pretrain_stats.pkl'), 'rb') as f:\n",
        "            pretrain_stats = pickle.load(f)\n",
        "        with open(os.path.join(opts.output_dir, 'train_stats.pkl'), 'rb') as f:\n",
        "            train_stats = pickle.load(f)\n",
        "    except: \n",
        "        print(\"Can't find training stats...\")\n",
        "        pretrain_stats, train_stats = None, None\n",
        "    print(\"*\"*50)\n",
        "\n",
        "    tokenizer.add_special_tokens({'additional_special_tokens': ['<|start|>', '<|p1|>', '<|p2|>', '<|act|>']})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    return model.to(device), tokenizer, pretrain_stats, train_stats\n",
        "\n",
        "\n",
        "model, tokenizer, pretrain_stats, train_stats = load_from_pretrained()\n",
        "p1_tok, p2_tok, start_tok = tokenizer.encode('<|p1|>')[0], tokenizer.encode('<|p2|>')[0], tokenizer.encode('<|start|>')[0]\n",
        "\n",
        "# new, action token\n",
        "act_tok = tokenizer.encode('<|act|>')[0]"
      ],
      "metadata": {
        "id": "8FAaUjClW2TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9EXjDML3a3G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Oct 12 14:17:28 2020\n",
        "\n",
        "@author: af1tang\n",
        "\"\"\"\n",
        "import torch, os, pickle, matplotlib.pyplot as plt\n",
        "import torch.nn as nn, torch.nn.functional as F\n",
        "from itertools import groupby\n",
        "\n",
        "## Utils ##\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "def chunker(seq, size):\n",
        "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
        "    \n",
        "def to_data(x):\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cpu()\n",
        "    return x.data.numpy()\n",
        "\n",
        "def to_var(x):\n",
        "    if not torch.is_tensor(x):\n",
        "        x = torch.Tensor(x)\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "    return x\n",
        "\n",
        "def process_conv(row, tokenizer, eos = True, make_flat=True):\n",
        "    if eos:\n",
        "        conv = list([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row])\n",
        "    else: conv = list([tokenizer.encode(x) for x in row])\n",
        "    if make_flat: conv = flatten(conv)\n",
        "    return conv\n",
        "\n",
        "def split_by_index(seq, sep):\n",
        "    result = []\n",
        "    for el in seq:\n",
        "        result.append(el)\n",
        "        if el == sep:\n",
        "            yield result\n",
        "            result = []\n",
        "            \n",
        "def filter_turn_indices(x):\n",
        "    filtered = [[t[1] for t in list(g)] for k,g in groupby(list(enumerate(x)), lambda x: x[1]==tokenizer.eos_token_id) if not k]\n",
        "    return filtered\n",
        "\n",
        "def display_dialog_history(dialog_hx):\n",
        "    for j, line in enumerate(dialog_hx):\n",
        "        msg = tokenizer.decode(line)\n",
        "        if j %2 == 0:\n",
        "            print(\">> User: \"+ msg)\n",
        "        else:\n",
        "            print(\"Bot: \"+msg)\n",
        "            print()"
      ],
      "metadata": {
        "id": "ca1koueGUsXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(model.parameters()).device"
      ],
      "metadata": {
        "id": "gofXaJnja3mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Oct 12 11:40:29 2020\n",
        "\n",
        "@author: af1tang\n",
        "\"\"\"\n",
        "from tqdm import tqdm\n",
        "import torch, os, pickle\n",
        "import torch.nn as nn, torch.nn.functional as F\n",
        "import numpy as np, random\n",
        "import pandas as pd\n",
        "\n",
        "## Persona Preprocess ##\n",
        "def preprocess_convai(filename):\n",
        "    raw_data = open(filename).read().strip().split('\\n')\n",
        "    data, count = {}, 0\n",
        "    curr_convo, curr_ps, curr_pt = [], [], []\n",
        "    indices = []\n",
        "    \n",
        "    person_a = 'your persona'\n",
        "    person_b = \"partner's persona\"\n",
        "    with tqdm(total = len(raw_data)) as pbar:\n",
        "        turn_count, ctx_count = 1,0 #init cycle\n",
        "        for idx, line in enumerate(raw_data):\n",
        "            if person_a in line[0:20]:\n",
        "                if (turn_count != 0) and (len(curr_ps)>1 and len(curr_pt)>1 and len(curr_convo)>1):\n",
        "                    if idx > 1:\n",
        "                        if curr_convo[0] == '__SILENCE__' :\n",
        "                            p1 = curr_ps; p2 = curr_pt; curr_convo = curr_convo[1:]\n",
        "                        else:\n",
        "                            p1 = curr_pt; p2 = curr_ps\n",
        "                        data[count] = { 'inp': process_conv([curr_convo[0]], tokenizer),\n",
        "                                        'labels': process_conv(curr_convo[1:],tokenizer), #to_data(torch.cat(curr_convo,dim=-1)[0]), \n",
        "                                       'p_src': process_conv(p1, tokenizer,make_flat=False), #to_data(torch.cat(curr_ps,dim=-1)[0]),\n",
        "                                       'p_trg': process_conv(p2, tokenizer, make_flat=False)}#to_data(torch.cat(curr_pt,dim=-1)[0])}\n",
        "                        count+=1\n",
        "                    curr_convo, curr_ps, curr_pt = [], [], []\n",
        "                    turn_count=0\n",
        "\n",
        "                words = line.split()\n",
        "                turn_id, words = int(words[0]), ' '.join(words[3:])\n",
        "                curr_ps.append(words)\n",
        "\n",
        "                ctx_count +=1\n",
        "                assert ctx_count == turn_id\n",
        "                \n",
        "            elif person_b in line[0:20]:\n",
        "                if (turn_count != 0) and (len(curr_ps)>1 and len(curr_pt)>1 and len(curr_convo)>1):\n",
        "                    if idx > 1:\n",
        "                        if curr_convo[0] == '__SILENCE__' :\n",
        "                            p1 = curr_ps; p2 = curr_pt; curr_convo = curr_convo[1:]\n",
        "                        else:\n",
        "                            p1 = curr_pt; p2 = curr_ps\n",
        "                        data[count] = { 'inp': process_conv([curr_convo[0]], tokenizer),\n",
        "                                        'labels': process_conv(curr_convo[1:],tokenizer), #to_data(torch.cat(curr_convo,dim=-1)[0]), \n",
        "                                       'p_src': process_conv(p1, tokenizer,make_flat=False), #to_data(torch.cat(curr_ps,dim=-1)[0]),\n",
        "                                       'p_trg': process_conv(p2, tokenizer, make_flat=False)}#to_data(torch.cat(curr_pt,dim=-1)[0])}\n",
        "                        count+=1\n",
        "                    curr_convo, curr_ps, curr_pt = [], [], []\n",
        "                    turn_count=0\n",
        "                words = line.split()\n",
        "                turn_id, words = int(words[0]), ' '.join(words[3:])\n",
        "                curr_pt.append(words)\n",
        "\n",
        "                ctx_count +=1\n",
        "                assert ctx_count == turn_id\n",
        "\n",
        "                \n",
        "            else:\n",
        "                if ctx_count !=0:\n",
        "                    turn_count = ctx_count *1 \n",
        "                    ctx_count =0\n",
        "                    indices.append(idx)\n",
        "                        \n",
        "                src_line, trg_line = line.split('\\t')\n",
        "                src_words = src_line.split()\n",
        "                src_idx, src_line = src_words[0], ' '.join(src_words[1:])\n",
        "\n",
        "                curr_convo.append(src_line) \n",
        "                curr_convo.append(trg_line)#turn)\n",
        "                \n",
        "                turn_count +=1\n",
        "                assert turn_count == int(src_idx)\n",
        "                \n",
        "            pbar.update(1)\n",
        "        \n",
        "    return data\n",
        "\n",
        "def convert_to_XY(old_data):\n",
        "    data = []\n",
        "    print(\"building training set...\")\n",
        "    for i in range(len(old_data)):\n",
        "        p1 = [tokenizer.decode(p) for p in old_data[i]['p_src']]\n",
        "        p2 = [tokenizer.decode(p) for p in old_data[i]['p_trg']]\n",
        "\n",
        "        convo = old_data[i]['inp'] + old_data[i]['labels']\n",
        "        dialog_hx = list(split_by_index(convo,tokenizer.eos_token_id))\n",
        "        #if len(dialog_hx) < 30:\n",
        "        dialog_hx = [line + [tokenizer.eos_token_id] for line in dialog_hx[:20]] # limit by max len of convo\n",
        "        p1_ctx = tokenizer.encode(''.join(['<|p1|>'] + p1 + ['<|sep|>'] + ['<|start|>']))\n",
        "        p2_ctx = tokenizer.encode(''.join(['<|p2|>'] + p2 + ['<|sep|>'] + ['<|start|>']))\n",
        "        for t in range(len(dialog_hx)):\n",
        "            x = dialog_hx[:t]\n",
        "            y = dialog_hx[t]\n",
        "            if t == 0:\n",
        "                x = p1_ctx[:-1] \n",
        "                y = [p1_ctx[-1]] + y\n",
        "            elif t %2 ==0:\n",
        "                x = p1_ctx + flatten(x)\n",
        "            else:\n",
        "                x = p2_ctx + flatten(x)\n",
        "            data.append((x,y))\n",
        "    return data\n",
        "\n",
        "def build_active_data():\n",
        "    df = pd.read_csv(os.path.join(data_path, 'active_learning_data.csv'))\n",
        "    X, y = df['context'].tolist(), df['response'].tolist()\n",
        "    X, y = [tokenizer.encode(x) for x in X], [tokenizer.encode(yy) for yy in y]\n",
        "    data = {'X':X, 'y':y}\n",
        "    return data\n",
        "    \n",
        "    \n",
        "if __name__ == '__main__':        \n",
        "    val_data = preprocess_convai('/content/personaGPT/data/valid_both_original_no_cands.txt')\n",
        "    val_data = convert_to_XY(val_data)\n",
        "    with open('/content/personaGPT/data/val_data', 'wb') as f: pickle.dump(val_data, f)"
      ],
      "metadata": {
        "id": "deqqkRMNbYK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from icecream import ic"
      ],
      "metadata": {
        "id": "tnbPD9IBdtmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score sacrebleu\n",
        "!pip install git+https://github.com/google-research/bleurt.git"
      ],
      "metadata": {
        "id": "arP_QOOvZGWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "rouge = evaluate.load('rouge')\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "bleurt = evaluate.load(\"bleurt\", module_type=\"metric\")\n",
        "meteor = evaluate.load('meteor')"
      ],
      "metadata": {
        "id": "ygwP_9hSXw7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_rouge = []\n",
        "avg_bleu = []\n",
        "avg_bert = []\n",
        "avg_sbleu = []\n",
        "avg_bleurt = []\n",
        "avg_meteor = []"
      ],
      "metadata": {
        "id": "yOxpegR_cn3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/personaGPT/data/val_data', 'rb') as f: data = pickle.load(f)\n",
        "dataloader = DataLoader(data, batch_size=1, shuffle=True); del data\n",
        "data_iter = iter(dataloader)\n",
        "with torch.no_grad():\n",
        "    eval_stats, total_steps, val_loss, val_f1_score = {}, 0, 0.0, 0.0\n",
        "    model.eval()\n",
        "    for i in tqdm(range(len(dataloader))):\n",
        "        batch = next(data_iter)\n",
        "        xx,yy = batch\n",
        "        try:\n",
        "            xx, yy = torch.stack(xx, -1).to(device), torch.stack(yy, -1).to(device)\n",
        "        except:\n",
        "            xx, yy = to_var(xx), to_var(yy)\n",
        "        ## forward on new data batch\n",
        "        dic_output = model(xx);\n",
        "        past = dic_output['past_key_values']\n",
        "        outp = model(yy, past_key_values=past, labels=yy)\n",
        "        loss = outp[0]\n",
        "        ytrue=np.array( filter_turn_indices(to_data(yy[...,1:].contiguous().view(-1)) ) )\n",
        "        ytext = tokenizer.decode(torch.LongTensor(ytrue[0]))\n",
        "        ypred=np.array( filter_turn_indices(to_data( outp[1][..., :-1, :].contiguous().topk(1)[1].view(-1)) ) ) \n",
        "        try:\n",
        "          yptext = tokenizer.decode(torch.LongTensor(ypred[0]))\n",
        "        except:\n",
        "          yptext = \"<pad>\"\n",
        "\n",
        "        # avg_rouge.append(rouge.compute(references=[ytext], predictions=[yptext])['rougeL'])\n",
        "        avg_bleu.append(bleu.compute(predictions=[yptext], references=[ytext])['bleu'])\n",
        "        avg_bert.append(bertscore.compute(predictions=[yptext], references=[ytext], lang='en')['f1'][0]) # 5m \n",
        "        avg_sbleu.append(sacrebleu.compute(predictions=[yptext], references=[ytext])['score']) # almost no time\n",
        "        avg_bleurt.append(bleurt.compute(predictions=[yptext], references=[ytext])['scores'][0]) # half n hour\n",
        "        avg_meteor.append(meteor.compute(predictions=[yptext], references=[ytext])['meteor']) # 10 minutes\n",
        "\n",
        "        min_len = min(len(ypred), len(ytrue))\n",
        "        hits = [set(ypred[i]).intersection(set(ytrue[i])) for i in range(min_len)]\n",
        "        prec = [len(hits[i])/len(ypred[i]) for i in range(min_len)]\n",
        "        rec = [len(hits[i])/len(ytrue[i]) for i in range(min_len)]\n",
        "        f1 = np.mean([2*(prec[i]*rec[i])/(prec[i] + rec[i]+1e-3) for i in range(min_len)])\n",
        "        val_f1_score += f1\n",
        "        val_loss += loss.mean().item()\n",
        "        total_steps +=1 \n",
        "        #if total_steps%100 ==0: print(\"... %d out of %d\"%(total_steps, len(dataloader)))\n",
        "        \n",
        "val_loss = val_loss / total_steps \n",
        "val_f1_score = val_f1_score / total_steps\n",
        "perplexity = torch.exp(torch.tensor(val_loss)).item()\n",
        "eval_stats = {'perplexity': perplexity,\n",
        "              'loss': val_loss,\n",
        "              'f1': val_f1_score,\n",
        "              # 'rougeL': sum(avg_rouge)/len(avg_rouge),\n",
        "              'bleu': sum(avg_bleu)/len(avg_bleu),\n",
        "              'bertscore': sum(avg_bert)/len(avg_bert),\n",
        "              'sacrebleu': sum(avg_sbleu)/len(avg_sbleu),\n",
        "              'bleurt': sum(avg_bleurt)/len(avg_bleurt),\n",
        "              'meteor': sum(avg_meteor)/len(avg_meteor)\n",
        "              }\n",
        "print(\"Done.\")\n",
        "print(eval_stats)"
      ],
      "metadata": {
        "id": "GEbUWjMVW0PC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}